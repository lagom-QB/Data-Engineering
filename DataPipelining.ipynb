{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import math\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import AdaBoostRegressor, RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# import sqlite3\n",
    "# from airflow import DAG\n",
    "# from airflow.utils.dates import days_ago\n",
    "# from airflow.operators.python_operator import PythonOperator\n",
    "\n",
    "import nbformat\n",
    "\n",
    "import warnings\n",
    "from typing import Optional, Tuple, Callable, Dict, Any, List, Union\n",
    "import fire\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "from catboost import CatBoostRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.linear_model import ElasticNet, Lasso, Ridge, SGDRegressor\n",
    "from sklearn.svm import SVR\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from sklearn.discriminant_analysis import StandardScaler\n",
    "\n",
    "from fastapi import FastAPI\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "import streamlit as st\n",
    "# Create a requirements.txt file with the necessary packages\n",
    "# !pip freeze > airflow/dags/requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.load_data import load_data_from_source\n",
    "from app.transform_data import transform_data_into_features_and_targets\n",
    "from app.baseline_model import train_baseline\n",
    "\n",
    "from app.train_models import train_model, save_best_model, training_process\n",
    "\n",
    "from app.model_prediction import predict_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- VARIABLES ----\n",
    "explanable_cols = ['Player','Nation','Pos','Squad','Age','Born','Starts','Min','Gls','Total_Att','Blocks_Blocks','Blocks_Sh','Blocks_Pass','Clr','Err','Touches_Touches','Touches_DefPen','Dribbles_Succ','Dribbles_Att','Dribbles_Mis','AerialDuels_Won','AerialDuels_Lost']\n",
    "file_loc       = 'airflow/dags/assets/matches-checkpoint.csv'\n",
    "spanish_squads = ['Sevilla', 'Sporting Huelva', 'Athletic Club', 'Levante Planas',\n",
    "                  'UDG Tenerife', 'Villarreal', 'Madrid CFF', 'Barcelona',\n",
    "                  'Atlético Madrid', 'Real Madrid', 'Alhama', 'Alavés',\n",
    "                  'Real Sociedad', 'Levante', 'Real Betis', 'Valencia']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 87 entries, 0 to 86\n",
      "Data columns (total 17 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   Wk         87 non-null     int64  \n",
      " 1   Day        87 non-null     object \n",
      " 2   Date       87 non-null     object \n",
      " 3   Time       87 non-null     object \n",
      " 4   Home       87 non-null     object \n",
      " 5   xGHome     87 non-null     float64\n",
      " 6   Score      87 non-null     object \n",
      " 7   xGAway     87 non-null     float64\n",
      " 8   Away       87 non-null     object \n",
      " 9   Home_id    87 non-null     object \n",
      " 10  Away_id    87 non-null     object \n",
      " 11  Match_id   87 non-null     object \n",
      " 12  League_id  87 non-null     int64  \n",
      " 13  xPHome     87 non-null     float64\n",
      " 14  xPAway     87 non-null     float64\n",
      " 15  ScoreHome  87 non-null     int64  \n",
      " 16  ScoreAway  87 non-null     int64  \n",
      "dtypes: float64(4), int64(4), object(9)\n",
      "memory usage: 12.2+ KB\n",
      "(87, 17) None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Wk</th>\n",
       "      <th>Day</th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>Home</th>\n",
       "      <th>xGHome</th>\n",
       "      <th>Score</th>\n",
       "      <th>xGAway</th>\n",
       "      <th>Away</th>\n",
       "      <th>Home_id</th>\n",
       "      <th>Away_id</th>\n",
       "      <th>Match_id</th>\n",
       "      <th>League_id</th>\n",
       "      <th>xPHome</th>\n",
       "      <th>xPAway</th>\n",
       "      <th>ScoreHome</th>\n",
       "      <th>ScoreAway</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>Sat</td>\n",
       "      <td>2022-09-17</td>\n",
       "      <td>12:00</td>\n",
       "      <td>Barcelona</td>\n",
       "      <td>1.9</td>\n",
       "      <td>2–0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>UDG Tenerife</td>\n",
       "      <td>15f49df1</td>\n",
       "      <td>4c088abe</td>\n",
       "      <td>4df3a732</td>\n",
       "      <td>230</td>\n",
       "      <td>2.40</td>\n",
       "      <td>0.40</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Sat</td>\n",
       "      <td>2022-09-17</td>\n",
       "      <td>12:00</td>\n",
       "      <td>Alavés</td>\n",
       "      <td>1.1</td>\n",
       "      <td>1–2</td>\n",
       "      <td>1.2</td>\n",
       "      <td>Madrid CFF</td>\n",
       "      <td>aa11fb42</td>\n",
       "      <td>89818574</td>\n",
       "      <td>87c755cd</td>\n",
       "      <td>230</td>\n",
       "      <td>1.29</td>\n",
       "      <td>1.43</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Sat</td>\n",
       "      <td>2022-09-17</td>\n",
       "      <td>16:00</td>\n",
       "      <td>Real Sociedad</td>\n",
       "      <td>0.7</td>\n",
       "      <td>2–0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>Villarreal</td>\n",
       "      <td>c21f25d3</td>\n",
       "      <td>7a7bef84</td>\n",
       "      <td>abfde9d9</td>\n",
       "      <td>230</td>\n",
       "      <td>1.68</td>\n",
       "      <td>0.87</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>Sat</td>\n",
       "      <td>2022-09-17</td>\n",
       "      <td>16:00</td>\n",
       "      <td>Real Madrid</td>\n",
       "      <td>1.6</td>\n",
       "      <td>2–0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>Valencia</td>\n",
       "      <td>54582b93</td>\n",
       "      <td>f96ff499</td>\n",
       "      <td>d0329f46</td>\n",
       "      <td>230</td>\n",
       "      <td>1.94</td>\n",
       "      <td>0.80</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>Sat</td>\n",
       "      <td>2022-09-17</td>\n",
       "      <td>18:00</td>\n",
       "      <td>Sevilla</td>\n",
       "      <td>1.1</td>\n",
       "      <td>1–3</td>\n",
       "      <td>1.4</td>\n",
       "      <td>Atlético Madrid</td>\n",
       "      <td>215d9026</td>\n",
       "      <td>b56c2667</td>\n",
       "      <td>f4452586</td>\n",
       "      <td>230</td>\n",
       "      <td>1.15</td>\n",
       "      <td>1.58</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Wk  Day        Date   Time           Home  xGHome Score  xGAway  \\\n",
       "0   2  Sat  2022-09-17  12:00      Barcelona     1.9   2–0     0.4   \n",
       "1   2  Sat  2022-09-17  12:00         Alavés     1.1   1–2     1.2   \n",
       "2   2  Sat  2022-09-17  16:00  Real Sociedad     0.7   2–0     0.3   \n",
       "3   2  Sat  2022-09-17  16:00    Real Madrid     1.6   2–0     0.8   \n",
       "4   2  Sat  2022-09-17  18:00        Sevilla     1.1   1–3     1.4   \n",
       "\n",
       "              Away   Home_id   Away_id  Match_id  League_id  xPHome  xPAway  \\\n",
       "0     UDG Tenerife  15f49df1  4c088abe  4df3a732        230    2.40    0.40   \n",
       "1       Madrid CFF  aa11fb42  89818574  87c755cd        230    1.29    1.43   \n",
       "2       Villarreal  c21f25d3  7a7bef84  abfde9d9        230    1.68    0.87   \n",
       "3         Valencia  54582b93  f96ff499  d0329f46        230    1.94    0.80   \n",
       "4  Atlético Madrid  215d9026  b56c2667  f4452586        230    1.15    1.58   \n",
       "\n",
       "   ScoreHome  ScoreAway  \n",
       "0          2          0  \n",
       "1          1          2  \n",
       "2          2          0  \n",
       "3          2          0  \n",
       "4          1          3  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = load_data_from_source()\n",
    "display(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform Data\n",
    "\n",
    "Transform the loaded data into _features_ and _target_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Alavés', 'Alhama', 'Athletic Club', 'Atlético Madrid',\n",
       "       'Barcelona', 'Levante', 'Levante Planas', 'Madrid CFF',\n",
       "       'Real Betis', 'Real Madrid', 'Real Sociedad', 'Sevilla',\n",
       "       'Sporting Huelva', 'UDG Tenerife', 'Valencia', 'Villarreal'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_home, target_home = transform_data_into_features_and_targets(df=data, score='Home')\n",
    "features_away, target_away = transform_data_into_features_and_targets(df=data, score='Away')\n",
    "\n",
    "# Remember that features_home and features_away are the same, so we can use either one\n",
    "# However, target_home and target_away are different, so we need to concatenate them in order to have the full target\n",
    "\n",
    "np.unique([data.Home.unique(), data.Away.unique()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model\n",
    "Establish a baseline performance against which to compare future better models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline error: 1.1481481481481484\n",
      "Baseline error: 1.5555555555555556\n"
     ]
    }
   ],
   "source": [
    "train_baseline(features_home, target_home)\n",
    "train_baseline(features_away, target_away)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>params</th>\n",
       "      <th>mae</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LinearRegression(n_jobs=1)</td>\n",
       "      <td>{'positive': False, 'n_jobs': 1, 'fit_intercep...</td>\n",
       "      <td>1.088379</td>\n",
       "      <td>0 days 00:00:05.154085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KernelRidge(degree=1, gamma=0.1, kernel='poly')</td>\n",
       "      <td>{'kernel': 'poly', 'gamma': 0.1, 'degree': 1, ...</td>\n",
       "      <td>1.119095</td>\n",
       "      <td>0 days 00:00:00.266998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ridge(alpha=10, solver='cholesky')</td>\n",
       "      <td>{'solver': 'cholesky', 'fit_intercept': True, ...</td>\n",
       "      <td>1.127626</td>\n",
       "      <td>0 days 00:00:00.261246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SGDRegressor(alpha=0.1, learning_rate='adaptiv...</td>\n",
       "      <td>{'shuffle': True, 'penalty': 'l2', 'loss': 'ep...</td>\n",
       "      <td>1.205640</td>\n",
       "      <td>0 days 00:00:00.256997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>XGBRegressor(base_score=None, booster=None, ca...</td>\n",
       "      <td>{'n_estimators': 200, 'max_depth': 4}</td>\n",
       "      <td>1.262268</td>\n",
       "      <td>0 days 00:00:02.541548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>([DecisionTreeRegressor(criterion='friedman_ms...</td>\n",
       "      <td>{'n_estimators': 200, 'max_depth': 2}</td>\n",
       "      <td>1.262769</td>\n",
       "      <td>0 days 00:00:04.589863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(DecisionTreeRegressor(max_depth=3, random_sta...</td>\n",
       "      <td>{'n_estimators': 100, 'learning_rate': 0.01}</td>\n",
       "      <td>1.303815</td>\n",
       "      <td>0 days 00:00:10.596427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(DecisionTreeRegressor(max_depth=2, max_featur...</td>\n",
       "      <td>{'n_estimators': 150, 'max_depth': 2}</td>\n",
       "      <td>1.356880</td>\n",
       "      <td>0 days 00:00:08.584825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ElasticNet(alpha=1)</td>\n",
       "      <td>{'l1_ratio': 0.5, 'fit_intercept': True, 'alph...</td>\n",
       "      <td>1.358025</td>\n",
       "      <td>0 days 00:00:00.244288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>DecisionTreeRegressor(max_depth=2)</td>\n",
       "      <td>{'max_depth': 2}</td>\n",
       "      <td>1.359954</td>\n",
       "      <td>0 days 00:00:00.259384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>KNeighborsRegressor(n_neighbors=9)</td>\n",
       "      <td>{'n_neighbors': 9}</td>\n",
       "      <td>1.407407</td>\n",
       "      <td>0 days 00:00:00.272671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Lasso(alpha=1)</td>\n",
       "      <td>{'fit_intercept': True, 'alpha': 1}</td>\n",
       "      <td>1.611111</td>\n",
       "      <td>0 days 00:00:00.192525</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                model  \\\n",
       "0                          LinearRegression(n_jobs=1)   \n",
       "1     KernelRidge(degree=1, gamma=0.1, kernel='poly')   \n",
       "2                  Ridge(alpha=10, solver='cholesky')   \n",
       "3   SGDRegressor(alpha=0.1, learning_rate='adaptiv...   \n",
       "4   XGBRegressor(base_score=None, booster=None, ca...   \n",
       "5   ([DecisionTreeRegressor(criterion='friedman_ms...   \n",
       "6   (DecisionTreeRegressor(max_depth=3, random_sta...   \n",
       "7   (DecisionTreeRegressor(max_depth=2, max_featur...   \n",
       "8                                 ElasticNet(alpha=1)   \n",
       "9                  DecisionTreeRegressor(max_depth=2)   \n",
       "10                 KNeighborsRegressor(n_neighbors=9)   \n",
       "11                                     Lasso(alpha=1)   \n",
       "\n",
       "                                               params       mae  \\\n",
       "0   {'positive': False, 'n_jobs': 1, 'fit_intercep...  1.088379   \n",
       "1   {'kernel': 'poly', 'gamma': 0.1, 'degree': 1, ...  1.119095   \n",
       "2   {'solver': 'cholesky', 'fit_intercept': True, ...  1.127626   \n",
       "3   {'shuffle': True, 'penalty': 'l2', 'loss': 'ep...  1.205640   \n",
       "4               {'n_estimators': 200, 'max_depth': 4}  1.262268   \n",
       "5               {'n_estimators': 200, 'max_depth': 2}  1.262769   \n",
       "6        {'n_estimators': 100, 'learning_rate': 0.01}  1.303815   \n",
       "7               {'n_estimators': 150, 'max_depth': 2}  1.356880   \n",
       "8   {'l1_ratio': 0.5, 'fit_intercept': True, 'alph...  1.358025   \n",
       "9                                    {'max_depth': 2}  1.359954   \n",
       "10                                 {'n_neighbors': 9}  1.407407   \n",
       "11                {'fit_intercept': True, 'alpha': 1}  1.611111   \n",
       "\n",
       "                     time  \n",
       "0  0 days 00:00:05.154085  \n",
       "1  0 days 00:00:00.266998  \n",
       "2  0 days 00:00:00.261246  \n",
       "3  0 days 00:00:00.256997  \n",
       "4  0 days 00:00:02.541548  \n",
       "5  0 days 00:00:04.589863  \n",
       "6  0 days 00:00:10.596427  \n",
       "7  0 days 00:00:08.584825  \n",
       "8  0 days 00:00:00.244288  \n",
       "9  0 days 00:00:00.259384  \n",
       "10 0 days 00:00:00.272671  \n",
       "11 0 days 00:00:00.192525  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "rankings_home = {}\n",
    "training_process(rankings_home, features_home, target_home)\n",
    "# train_model(rankings_home,'LinearRegression', features_home, target_home)\n",
    "# train_model(rankings_home,'AdaBoostRegressor', features_home, target_home)\n",
    "# train_model(rankings_home,'RandomForestRegressor', features_home, target_home)\n",
    "# train_model(rankings_home,'XGBRegressor', features_home, target_home)\n",
    "# # train_model(rankings_home,'DecisionTreeRegressor', features_home, target_home)\n",
    "# # train_model(rankings_home,'KNeighborsRegressor', features_home, target_home)\n",
    "# # train_model(rankings_home,'GradientBoostingRegressor', features_home, target_home)\n",
    "# # train_model(rankings_home,'Ridge', features_home, target_home)\n",
    "# # train_model(rankings_home,'Lasso', features_home, target_home)\n",
    "# # train_model(rankings_home,'ElasticNet', features_home, target_home)\n",
    "# # train_model(rankings_home,'SGDRegressor', features_home, target_home)\n",
    "# # train_model(rankings_home,'KernelRidge', features_home, target_home)\n",
    "\n",
    "# print(sorted(rankings_home.items(), key=lambda x: x[1]['mae']))\n",
    "\n",
    "# Save models and their info\n",
    "save_best_model('home',rankings_home)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/iffiness/.pyenv/versions/3.10.12/envs/virtual_airflow/lib/python3.10/site-packages/sklearn/linear_model/_ridge.py:239: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\n",
      "/Users/iffiness/.pyenv/versions/3.10.12/envs/virtual_airflow/lib/python3.10/site-packages/sklearn/linear_model/_ridge.py:239: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\n",
      "/Users/iffiness/.pyenv/versions/3.10.12/envs/virtual_airflow/lib/python3.10/site-packages/sklearn/linear_model/_ridge.py:239: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\n",
      "/Users/iffiness/.pyenv/versions/3.10.12/envs/virtual_airflow/lib/python3.10/site-packages/sklearn/linear_model/_ridge.py:239: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\n",
      "/Users/iffiness/.pyenv/versions/3.10.12/envs/virtual_airflow/lib/python3.10/site-packages/sklearn/linear_model/_ridge.py:239: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\n",
      "/Users/iffiness/.pyenv/versions/3.10.12/envs/virtual_airflow/lib/python3.10/site-packages/sklearn/linear_model/_ridge.py:239: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\n",
      "/Users/iffiness/.pyenv/versions/3.10.12/envs/virtual_airflow/lib/python3.10/site-packages/sklearn/linear_model/_ridge.py:239: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\n",
      "/Users/iffiness/.pyenv/versions/3.10.12/envs/virtual_airflow/lib/python3.10/site-packages/sklearn/linear_model/_ridge.py:239: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\n",
      "/Users/iffiness/.pyenv/versions/3.10.12/envs/virtual_airflow/lib/python3.10/site-packages/sklearn/linear_model/_ridge.py:239: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\n",
      "/Users/iffiness/.pyenv/versions/3.10.12/envs/virtual_airflow/lib/python3.10/site-packages/sklearn/linear_model/_ridge.py:239: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('KNeighborsRegressor', {'model': KNeighborsRegressor(n_neighbors=4), 'params': {'n_neighbors': 4}, 'mae': 0.75, 'time': datetime.timedelta(microseconds=213178)}), ('KernelRidge', {'model': KernelRidge(gamma=0.1, kernel='rbf'), 'params': {'kernel': 'rbf', 'gamma': 0.1, 'degree': 3, 'alpha': 1}, 'mae': 0.8088054038390681, 'time': datetime.timedelta(microseconds=292240)}), ('AdaBoostRegressor', {'model': AdaBoostRegressor(learning_rate=0.01), 'params': {'n_estimators': 50, 'learning_rate': 0.01}, 'mae': 0.9097736385562363, 'time': datetime.timedelta(seconds=8, microseconds=658844)}), ('ElasticNet', {'model': ElasticNet(alpha=1, l1_ratio=0.9), 'params': {'l1_ratio': 0.9, 'fit_intercept': True, 'alpha': 1}, 'mae': 0.9219185482824981, 'time': datetime.timedelta(microseconds=218785)}), ('Lasso', {'model': Lasso(alpha=1), 'params': {'fit_intercept': True, 'alpha': 1}, 'mae': 0.9353056410838394, 'time': datetime.timedelta(microseconds=185569)}), ('Ridge', {'model': Ridge(alpha=1, solver='sag'), 'params': {'solver': 'sag', 'fit_intercept': True, 'alpha': 1}, 'mae': 0.9375560822674994, 'time': datetime.timedelta(microseconds=224072)}), ('LinearRegression', {'model': LinearRegression(n_jobs=1, positive=True), 'params': {'positive': True, 'n_jobs': 1, 'fit_intercept': True}, 'mae': 0.9397045250385171, 'time': datetime.timedelta(microseconds=226967)}), ('RandomForestRegressor', {'model': RandomForestRegressor(max_depth=2), 'params': {'n_estimators': 100, 'max_depth': 2}, 'mae': 0.9564978722203618, 'time': datetime.timedelta(seconds=10, microseconds=234840)}), ('SGDRegressor', {'model': SGDRegressor(alpha=0.01, learning_rate='adaptive', loss='epsilon_insensitive',\n",
      "             penalty='l1', shuffle=False), 'params': {'shuffle': False, 'penalty': 'l1', 'loss': 'epsilon_insensitive', 'learning_rate': 'adaptive', 'fit_intercept': True, 'alpha': 0.01}, 'mae': 0.9760116944262884, 'time': datetime.timedelta(microseconds=312049)}), ('DecisionTreeRegressor', {'model': DecisionTreeRegressor(max_depth=3), 'params': {'max_depth': 3}, 'mae': 0.9941520467836256, 'time': datetime.timedelta(microseconds=200815)}), ('XGBRegressor', {'model': XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "             gamma=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "             max_delta_step=None, max_depth=10, max_leaves=None,\n",
      "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "             multi_strategy=None, n_estimators=50, n_jobs=None,\n",
      "             num_parallel_tree=None, random_state=None, ...), 'params': {'n_estimators': 50, 'max_depth': 10}, 'mae': 1.0145864519808028, 'time': datetime.timedelta(seconds=2, microseconds=70060)}), ('GradientBoostingRegressor', {'model': GradientBoostingRegressor(max_depth=5, n_estimators=50), 'params': {'n_estimators': 50, 'max_depth': 5}, 'mae': 1.0531503445586592, 'time': datetime.timedelta(seconds=4, microseconds=257429)})]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>params</th>\n",
       "      <th>mae</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KNeighborsRegressor(n_neighbors=4)</td>\n",
       "      <td>{'n_neighbors': 4}</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0 days 00:00:00.213178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KernelRidge(gamma=0.1, kernel='rbf')</td>\n",
       "      <td>{'kernel': 'rbf', 'gamma': 0.1, 'degree': 3, '...</td>\n",
       "      <td>0.808805</td>\n",
       "      <td>0 days 00:00:00.292240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(DecisionTreeRegressor(max_depth=3, random_sta...</td>\n",
       "      <td>{'n_estimators': 50, 'learning_rate': 0.01}</td>\n",
       "      <td>0.909774</td>\n",
       "      <td>0 days 00:00:08.658844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ElasticNet(alpha=1, l1_ratio=0.9)</td>\n",
       "      <td>{'l1_ratio': 0.9, 'fit_intercept': True, 'alph...</td>\n",
       "      <td>0.921919</td>\n",
       "      <td>0 days 00:00:00.218785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lasso(alpha=1)</td>\n",
       "      <td>{'fit_intercept': True, 'alpha': 1}</td>\n",
       "      <td>0.935306</td>\n",
       "      <td>0 days 00:00:00.185569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Ridge(alpha=1, solver='sag')</td>\n",
       "      <td>{'solver': 'sag', 'fit_intercept': True, 'alph...</td>\n",
       "      <td>0.937556</td>\n",
       "      <td>0 days 00:00:00.224072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LinearRegression(n_jobs=1, positive=True)</td>\n",
       "      <td>{'positive': True, 'n_jobs': 1, 'fit_intercept...</td>\n",
       "      <td>0.939705</td>\n",
       "      <td>0 days 00:00:00.226967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(DecisionTreeRegressor(max_depth=2, max_featur...</td>\n",
       "      <td>{'n_estimators': 100, 'max_depth': 2}</td>\n",
       "      <td>0.956498</td>\n",
       "      <td>0 days 00:00:10.234840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>SGDRegressor(alpha=0.01, learning_rate='adapti...</td>\n",
       "      <td>{'shuffle': False, 'penalty': 'l1', 'loss': 'e...</td>\n",
       "      <td>0.976012</td>\n",
       "      <td>0 days 00:00:00.312049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>DecisionTreeRegressor(max_depth=3)</td>\n",
       "      <td>{'max_depth': 3}</td>\n",
       "      <td>0.994152</td>\n",
       "      <td>0 days 00:00:00.200815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>XGBRegressor(base_score=None, booster=None, ca...</td>\n",
       "      <td>{'n_estimators': 50, 'max_depth': 10}</td>\n",
       "      <td>1.014586</td>\n",
       "      <td>0 days 00:00:02.070060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>([DecisionTreeRegressor(criterion='friedman_ms...</td>\n",
       "      <td>{'n_estimators': 50, 'max_depth': 5}</td>\n",
       "      <td>1.053150</td>\n",
       "      <td>0 days 00:00:04.257429</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                model  \\\n",
       "0                  KNeighborsRegressor(n_neighbors=4)   \n",
       "1                KernelRidge(gamma=0.1, kernel='rbf')   \n",
       "2   (DecisionTreeRegressor(max_depth=3, random_sta...   \n",
       "3                   ElasticNet(alpha=1, l1_ratio=0.9)   \n",
       "4                                      Lasso(alpha=1)   \n",
       "5                        Ridge(alpha=1, solver='sag')   \n",
       "6           LinearRegression(n_jobs=1, positive=True)   \n",
       "7   (DecisionTreeRegressor(max_depth=2, max_featur...   \n",
       "8   SGDRegressor(alpha=0.01, learning_rate='adapti...   \n",
       "9                  DecisionTreeRegressor(max_depth=3)   \n",
       "10  XGBRegressor(base_score=None, booster=None, ca...   \n",
       "11  ([DecisionTreeRegressor(criterion='friedman_ms...   \n",
       "\n",
       "                                               params       mae  \\\n",
       "0                                  {'n_neighbors': 4}  0.750000   \n",
       "1   {'kernel': 'rbf', 'gamma': 0.1, 'degree': 3, '...  0.808805   \n",
       "2         {'n_estimators': 50, 'learning_rate': 0.01}  0.909774   \n",
       "3   {'l1_ratio': 0.9, 'fit_intercept': True, 'alph...  0.921919   \n",
       "4                 {'fit_intercept': True, 'alpha': 1}  0.935306   \n",
       "5   {'solver': 'sag', 'fit_intercept': True, 'alph...  0.937556   \n",
       "6   {'positive': True, 'n_jobs': 1, 'fit_intercept...  0.939705   \n",
       "7               {'n_estimators': 100, 'max_depth': 2}  0.956498   \n",
       "8   {'shuffle': False, 'penalty': 'l1', 'loss': 'e...  0.976012   \n",
       "9                                    {'max_depth': 3}  0.994152   \n",
       "10              {'n_estimators': 50, 'max_depth': 10}  1.014586   \n",
       "11               {'n_estimators': 50, 'max_depth': 5}  1.053150   \n",
       "\n",
       "                     time  \n",
       "0  0 days 00:00:00.213178  \n",
       "1  0 days 00:00:00.292240  \n",
       "2  0 days 00:00:08.658844  \n",
       "3  0 days 00:00:00.218785  \n",
       "4  0 days 00:00:00.185569  \n",
       "5  0 days 00:00:00.224072  \n",
       "6  0 days 00:00:00.226967  \n",
       "7  0 days 00:00:10.234840  \n",
       "8  0 days 00:00:00.312049  \n",
       "9  0 days 00:00:00.200815  \n",
       "10 0 days 00:00:02.070060  \n",
       "11 0 days 00:00:04.257429  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rankings_away = {}\n",
    "training_process(rankings_away, features_away, target_away)\n",
    "# train_model(rankings_away, 'LinearRegression', features_away, target_away)\n",
    "# train_model(rankings_away, 'AdaBoostRegressor', features_away, target_away)\n",
    "# train_model(rankings_away, 'RandomForestRegressor', features_away, target_away)\n",
    "# train_model(rankings_away, 'XGBRegressor', features_away, target_away)\n",
    "# train_model(rankings_away, 'DecisionTreeRegressor', features_away, target_away)\n",
    "# train_model(rankings_away, 'KNeighborsRegressor', features_away, target_away)\n",
    "# train_model(rankings_away, 'GradientBoostingRegressor', features_away, target_away)\n",
    "# train_model(rankings_away, 'Ridge', features_away, target_away)\n",
    "# train_model(rankings_away, 'Lasso', features_away, target_away)\n",
    "# train_model(rankings_away, 'ElasticNet', features_away, target_away)\n",
    "# train_model(rankings_away, 'SGDRegressor', features_away, target_away)\n",
    "# train_model(rankings_away, 'KernelRidge', features_away, target_away)\n",
    "\n",
    "print(sorted(rankings_away.items(), key=lambda x: x[1]['mae']))\n",
    "\n",
    "# Save models and their info\n",
    "save_best_model('away',rankings_away)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Models from Pickle file\n",
    "\n",
    "Load the models from the pickle to a REST API. I'm going to use _fastapi_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- PREDICTIONS ----\n",
    "\n",
    "cols: list[str] = ['Wk', 'Numeric_Day', 'Numeric_Home', 'Numeric_Away', 'Numeric_Time',\n",
    "       'xGHome_xGAway_1', 'xGHome_xGAway_2', 'xGHome_xGAway_3']\n",
    "# display(features_away[cols].sample(1))\n",
    "\n",
    "# Pick random number between 0 and len(data)\n",
    "random_index = np.random.randint(0, len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'predicting ... 8 : 1'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display(features_away[cols][random_index:random_index+1], features_home[cols][random_index:random_index+1])\n",
    "# Predict home score for match at random index\n",
    "predict_res(features_home[cols][random_index:random_index+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'predicting ... 8 : 1'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_res(features_away[cols][random_index:random_index+1])\n",
    "\n",
    "# The prediction is the same for both because the features are the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Streamlit to transform the data, load the model and do a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2448822055.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[12], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    print(..)\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "print(..)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ETL Process and Data Integration\n",
    "\n",
    "__Apache Airflow__ supports a few databases: \n",
    "- SQLite _Lightweight filebased database suitable for small-scale deployments and testing_\n",
    "- PostgreSQL _Relational database widely used in production environments_\n",
    "- MySQL _Popular relational database widely used_\n",
    "- Microsoft SQL Server _Commercial relational database widely used in enterprises_\n",
    "- Oracle _Commercial relational database widely used in enterprises_\n",
    "- Amazon RedShift _Cloud-based data warehouse optimized for analytics workloads_\n",
    "- Google BigQuery _Cloud-based data warehouse optimized for analytics workloads_\n",
    "- Apache Casssandra _Distributed No-SQL database optimized for high scalability and availability_\n",
    "- Apache Hive _Data warehouse infrastructure for data summarization, querying and analytics_\n",
    "\n",
    "I'm using SQLite because it's a small scale dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load task\n",
    "def load_data(matches):\n",
    "    # Connect to database\n",
    "    conn = sqlite3.connect('assets/spanish_matches.db')\n",
    "\n",
    "    # Create cursor\n",
    "    c = conn.cursor()\n",
    "\n",
    "    # Create table\n",
    "    c.execute(\"\"\"CREATE TABLE IF NOT EXISTS matches (\n",
    "        Wk INTERGER,\n",
    "        Day TEXT,\n",
    "        Date DATE,\n",
    "        Time TIME,\n",
    "        Home TEXT,\n",
    "        xGHome FLOAT,\n",
    "        Score TEXT,\n",
    "        xGAway FLOAT,\n",
    "        Away TEXT,\n",
    "        xPHome FLOAT,\n",
    "        xPAway FLOAT,\n",
    "        ScoreHome INTERGER,\n",
    "        ScoreAway INTERGER,\n",
    "        GoalDifference INTERGER,\n",
    "        Result TEXT,\n",
    "        ExpectedGoalDifference FLOAT,\n",
    "        Points INTERGER,\n",
    "        ExpectedPoints INTERGER,\n",
    "        WinPercentage FLOAT,\n",
    "        TotalGoals INTERGER,\n",
    "        xGRatio FLOAT\n",
    "    )\"\"\")\n",
    "\n",
    "    # Insert DataFrame records one by one.\n",
    "    for i, row in matches.iterrows():\n",
    "        c.execute(\"\"\"INSERT INTO matches VALUES (?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?)\"\"\", (\n",
    "            row['Wk'],\n",
    "            row['Day'],\n",
    "            row['Date'],\n",
    "            row['Time'],\n",
    "            row['Home'],\n",
    "            row['xGHome'],\n",
    "            row['Score'],\n",
    "            row['xGAway'],\n",
    "            row['Away'],\n",
    "            row['xPHome'],\n",
    "            row['xPAway'],\n",
    "            row['ScoreHome'],\n",
    "            row['ScoreAway'],\n",
    "            row['GoalDifference'],\n",
    "            row['Result'],\n",
    "            row['ExpectedGoalDifference'],\n",
    "            row['Points'],\n",
    "            row['ExpectedPoints'],\n",
    "            row['WinPercentage'],\n",
    "            row['TotalGoals'],\n",
    "            row['xGRatio']\n",
    "        ))\n",
    "\n",
    "    # Commit changes\n",
    "    conn.commit()\n",
    "\n",
    "    # Close cursor and connection\n",
    "    c.close()\n",
    "    conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign tasks\n",
    "extract_task = PythonOperator(\n",
    "    task_id='extract_data',\n",
    "    python_callable=extract_data,\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "transform_task = PythonOperator(\n",
    "    task_id='transform_data',\n",
    "    python_callable=transform_data,\n",
    "    op_kwargs={'matches': '{{ ti.xcom_pull(task_ids=\"extract_data\") }}'},\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "load_task = PythonOperator(\n",
    "    task_id='load_data',\n",
    "    python_callable=load_data,\n",
    "    op_kwargs={'matches': '{{ ti.xcom_pull(task_ids=\"transform_data\") }}'},\n",
    "    dag=dag\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!~/airflow_env/bin/airflow scheduler -D\n",
    "!~/airflow_env/bin/airflow webserver -D\n",
    "\n",
    "!~/airflow_env/bin/airflow dags list\n",
    "\n",
    "!~/airflow_env/bin/airflow cheat-sheet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation and Quality Assurance\n",
    "\n",
    "To validate the quality of the data, I'm connecting to the database to check for null values in each column of the matches table.  \n",
    "Specifically, I'll check the data type, the range anf completeness of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate data in the database and ensure the proper quality\n",
    "def validate_data():\n",
    "    # Connect to database\n",
    "    conn = sqlite3.connect('assets/spanish_matches.db')\n",
    "\n",
    "    # Create cursor\n",
    "    c = conn.cursor()\n",
    "\n",
    "    # Data type validation\n",
    "    c.execute(\"\"\"SELECT COUNT(*) FROM matches where CAST(Wk AS INTEGER) IS NULL\"\"\")\n",
    "    null_count = c.fetchone()[0]\n",
    "    if null_count == 0:\n",
    "        print('Data type validation passed.')\n",
    "    else:\n",
    "        print(f'Data type validation failed with {null_count} null values.')\n",
    "\n",
    "    # Data range validation\n",
    "    c.execute(\"\"\"SELECT COUNT(*) FROM matches where Wk < 1 OR Wk > 10\"\"\")\n",
    "    range_count = c.fetchone()[0]\n",
    "    if range_count == 0:\n",
    "        print('Data range validation passed.')\n",
    "    else:\n",
    "        print(f'Data range validation failed with {range_count} values out of range.')\n",
    "\n",
    "    # Data completeness validation\n",
    "    c.execute(\"\"\"SELECT COUNT(*) FROM matches where Wk IS NULL\"\"\")\n",
    "    completeness_count = c.fetchone()[0]\n",
    "    if completeness_count == 0:\n",
    "        print('Data completeness validation passed.')\n",
    "    else:\n",
    "        print(f'Data completeness validation failed with {completeness_count} null values.')\n",
    "\n",
    "    c.close()\n",
    "    conn.close()\n",
    "\n",
    "validate_task = PythonOperator(\n",
    "    task_id='validate_data',\n",
    "    python_callable=validate_data,\n",
    "    op_kwargs={'matches': '{{ ti.xcom_pull(task_ids=\"load_data\") }}'},\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "# Define task dependencies\n",
    "extract_task >> transform_task >> load_task >> validate_task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reporting and Analysis\n",
    "\n",
    "Generate meaningful insights and reports.\n",
    "- Trend analysis\n",
    "- Team Performance analysis\n",
    "- Team comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trend analysis\n",
    "def trend_analysis():\n",
    "    # Connect to database\n",
    "    conn = sqlite3.connect('assets/spanish_matches.db')\n",
    "\n",
    "    # Create cursor\n",
    "    c = conn.cursor()\n",
    "\n",
    "    # Data type validation\n",
    "    c.execute(\"\"\"SELECT * FROM matches\"\"\")\n",
    "    matches = pd.DataFrame(c.fetchall())\n",
    "    \n",
    "    # Define plot function\n",
    "    def plot_data():\n",
    "        sns.lineplot(x='Date', y='TotlaGoals', data=matches)\n",
    "        plt.title('Total Goals Scored')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Total Goals')\n",
    "        plt.show()\n",
    "    # Look at the correlation between the expected goals and the actual goals\n",
    "    def calculate_correlation():\n",
    "        corr_home = matches['xGHome'].corr(matches['ScoreHome'])\n",
    "        print(f'Correlation between expected Goals for the Home and actual goals Home: {corr_home}')\n",
    "        corr_away = matches['xGAway'].corr(matches['ScoreAway'])\n",
    "        print(f'Correlation between expected Goals for the Away and actual goals Away: {corr_away}')\n",
    "    \n",
    "    plot_data()\n",
    "    calculate_correlation()\n",
    "\n",
    "trend_analysis_task = PythonOperator(\n",
    "    task_id='trend_analysis',\n",
    "    python_callable=trend_analysis,\n",
    "    op_kwargs={'matches': '{{ ti.xcom_pull(task_ids=\"validate_data\") }}'},\n",
    "    dag=dag\n",
    ")   \n",
    "\n",
    "# Define task dependencies\n",
    "extract_task >> transform_task >> load_task >> validate_task >> trend_analysis_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exectute the DAG workflow and view the results in the Airflow UI from scripts/DataPipelining.py\n",
    "!~/airflow_env/bin/airflow trigger_dag "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert notebook to python script\n",
    "!jupyter nbconvert --to script DataPipelining.ipynb --output-dir='airflow/dags/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
