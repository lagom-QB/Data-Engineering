{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy\n",
      "  Obtaining dependency information for numpy from https://files.pythonhosted.org/packages/be/f8/034752c5131c46e10364e4db241974f2eb6bb31bbfc4335344c19e17d909/numpy-1.26.0-cp310-cp310-macosx_10_9_x86_64.whl.metadata\n",
      "  Using cached numpy-1.26.0-cp310-cp310-macosx_10_9_x86_64.whl.metadata (53 kB)\n",
      "Collecting pandas\n",
      "  Obtaining dependency information for pandas from https://files.pythonhosted.org/packages/f3/e6/7021570b1152ae8efc2dc99f4aef2c0b91c1f098a18cb8671d5b06ebdf53/pandas-2.1.1-cp310-cp310-macosx_10_9_x86_64.whl.metadata\n",
      "  Using cached pandas-2.1.1-cp310-cp310-macosx_10_9_x86_64.whl.metadata (18 kB)\n",
      "Collecting matplotlib\n",
      "  Obtaining dependency information for matplotlib from https://files.pythonhosted.org/packages/30/5b/a6214caaa5adf07b52aecba98fdace32cc51e63a1fcc1f98d60ec128a6c0/matplotlib-3.8.0-cp310-cp310-macosx_10_12_x86_64.whl.metadata\n",
      "  Using cached matplotlib-3.8.0-cp310-cp310-macosx_10_12_x86_64.whl.metadata (5.8 kB)\n",
      "Collecting seaborn\n",
      "  Obtaining dependency information for seaborn from https://files.pythonhosted.org/packages/7b/e5/83fcd7e9db036c179e0352bfcd20f81d728197a16f883e7b90307a88e65e/seaborn-0.13.0-py3-none-any.whl.metadata\n",
      "  Using cached seaborn-0.13.0-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting scikit-learn\n",
      "  Obtaining dependency information for scikit-learn from https://files.pythonhosted.org/packages/74/17/01347a4e2298edd6d152b79d4d042f902a618ccaf4c070f0a61999a26156/scikit_learn-1.3.1-cp310-cp310-macosx_10_9_x86_64.whl.metadata\n",
      "  Using cached scikit_learn-1.3.1-cp310-cp310-macosx_10_9_x86_64.whl.metadata (11 kB)\n",
      "Collecting datetime\n",
      "  Obtaining dependency information for datetime from https://files.pythonhosted.org/packages/95/88/3b9d4042b396221a132180b392ab2a174031a6fb579f7927f3909fc183a7/DateTime-5.2-py3-none-any.whl.metadata\n",
      "  Using cached DateTime-5.2-py3-none-any.whl.metadata (33 kB)\n",
      "Collecting nbformat\n",
      "  Obtaining dependency information for nbformat from https://files.pythonhosted.org/packages/f4/e7/ef30a90b70eba39e675689b9eaaa92530a71d7435ab8f9cae520814e0caf/nbformat-5.9.2-py3-none-any.whl.metadata\n",
      "  Using cached nbformat-5.9.2-py3-none-any.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./data_engineering/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./data_engineering/lib/python3.10/site-packages (from pandas) (2023.3.post1)\n",
      "Collecting tzdata>=2022.1 (from pandas)\n",
      "  Using cached tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Obtaining dependency information for contourpy>=1.0.1 from https://files.pythonhosted.org/packages/fb/7f/c44a51a83a093bf5c84e07dd1e3cfe9f68c47b6499bd05a9de0c6dbdc2bc/contourpy-1.1.1-cp310-cp310-macosx_10_9_x86_64.whl.metadata\n",
      "  Using cached contourpy-1.1.1-cp310-cp310-macosx_10_9_x86_64.whl.metadata (5.9 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Obtaining dependency information for cycler>=0.10 from https://files.pythonhosted.org/packages/e7/05/c19819d5e3d95294a6f5947fb9b9629efb316b96de511b418c53d245aae6/cycler-0.12.1-py3-none-any.whl.metadata\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Obtaining dependency information for fonttools>=4.22.0 from https://files.pythonhosted.org/packages/12/20/dd3ee8dd2bc4e6bfbcd3ad4d1d79e9dba0dcc287989380ae8b520b19b4a4/fonttools-4.43.1-cp310-cp310-macosx_10_9_x86_64.whl.metadata\n",
      "  Using cached fonttools-4.43.1-cp310-cp310-macosx_10_9_x86_64.whl.metadata (152 kB)\n",
      "Collecting kiwisolver>=1.0.1 (from matplotlib)\n",
      "  Obtaining dependency information for kiwisolver>=1.0.1 from https://files.pythonhosted.org/packages/0e/c1/d084f8edb26533a191415d5173157080837341f9a06af9dd1a75f727abb4/kiwisolver-1.4.5-cp310-cp310-macosx_10_9_x86_64.whl.metadata\n",
      "  Using cached kiwisolver-1.4.5-cp310-cp310-macosx_10_9_x86_64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in ./data_engineering/lib/python3.10/site-packages (from matplotlib) (23.2)\n",
      "Collecting pillow>=6.2.0 (from matplotlib)\n",
      "  Obtaining dependency information for pillow>=6.2.0 from https://files.pythonhosted.org/packages/8c/34/4e02804420b1cd9371ce5f7c0da7024a8450e6b092c336d3233d6c6448d9/Pillow-10.0.1-cp310-cp310-macosx_10_10_x86_64.whl.metadata\n",
      "  Using cached Pillow-10.0.1-cp310-cp310-macosx_10_10_x86_64.whl.metadata (9.5 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Obtaining dependency information for pyparsing>=2.3.1 from https://files.pythonhosted.org/packages/39/92/8486ede85fcc088f1b3dba4ce92dd29d126fd96b0008ea213167940a2475/pyparsing-3.1.1-py3-none-any.whl.metadata\n",
      "  Using cached pyparsing-3.1.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting scipy>=1.5.0 (from scikit-learn)\n",
      "  Obtaining dependency information for scipy>=1.5.0 from https://files.pythonhosted.org/packages/6d/bc/6f79da3a8edf5f432ccdc49fd35e8b4fe2ce1d4ad3b5360c742101a57838/scipy-1.11.3-cp310-cp310-macosx_10_9_x86_64.whl.metadata\n",
      "  Using cached scipy-1.11.3-cp310-cp310-macosx_10_9_x86_64.whl.metadata (60 kB)\n",
      "Collecting joblib>=1.1.1 (from scikit-learn)\n",
      "  Obtaining dependency information for joblib>=1.1.1 from https://files.pythonhosted.org/packages/10/40/d551139c85db202f1f384ba8bcf96aca2f329440a844f924c8a0040b6d02/joblib-1.3.2-py3-none-any.whl.metadata\n",
      "  Using cached joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn)\n",
      "  Obtaining dependency information for threadpoolctl>=2.0.0 from https://files.pythonhosted.org/packages/81/12/fd4dea011af9d69e1cad05c75f3f7202cdcbeac9b712eea58ca779a72865/threadpoolctl-3.2.0-py3-none-any.whl.metadata\n",
      "  Using cached threadpoolctl-3.2.0-py3-none-any.whl.metadata (10.0 kB)\n",
      "Collecting zope.interface (from datetime)\n",
      "  Obtaining dependency information for zope.interface from https://files.pythonhosted.org/packages/3c/ec/c1e7ce928dc10bfe02c6da7e964342d941aaf168f96f8084636167ea50d2/zope.interface-6.1-cp310-cp310-macosx_10_9_x86_64.whl.metadata\n",
      "  Using cached zope.interface-6.1-cp310-cp310-macosx_10_9_x86_64.whl.metadata (41 kB)\n",
      "Collecting fastjsonschema (from nbformat)\n",
      "  Obtaining dependency information for fastjsonschema from https://files.pythonhosted.org/packages/7f/1a/8aad366cf1779351741e5c791ae76dc8b293f72e9448c689cc2e730f06cb/fastjsonschema-2.18.1-py3-none-any.whl.metadata\n",
      "  Using cached fastjsonschema-2.18.1-py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: jsonschema>=2.6 in ./data_engineering/lib/python3.10/site-packages (from nbformat) (4.19.1)\n",
      "Requirement already satisfied: jupyter-core in ./data_engineering/lib/python3.10/site-packages (from nbformat) (5.4.0)\n",
      "Requirement already satisfied: traitlets>=5.1 in ./data_engineering/lib/python3.10/site-packages (from nbformat) (5.11.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in ./data_engineering/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in ./data_engineering/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in ./data_engineering/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in ./data_engineering/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat) (0.10.6)\n",
      "Requirement already satisfied: six>=1.5 in ./data_engineering/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in ./data_engineering/lib/python3.10/site-packages (from jupyter-core->nbformat) (3.11.0)\n",
      "Requirement already satisfied: setuptools in ./data_engineering/lib/python3.10/site-packages (from zope.interface->datetime) (68.2.2)\n",
      "Using cached numpy-1.26.0-cp310-cp310-macosx_10_9_x86_64.whl (20.6 MB)\n",
      "Using cached pandas-2.1.1-cp310-cp310-macosx_10_9_x86_64.whl (11.7 MB)\n",
      "Using cached matplotlib-3.8.0-cp310-cp310-macosx_10_12_x86_64.whl (7.6 MB)\n",
      "Using cached seaborn-0.13.0-py3-none-any.whl (294 kB)\n",
      "Using cached scikit_learn-1.3.1-cp310-cp310-macosx_10_9_x86_64.whl (10.2 MB)\n",
      "Using cached DateTime-5.2-py3-none-any.whl (52 kB)\n",
      "Using cached nbformat-5.9.2-py3-none-any.whl (77 kB)\n",
      "Using cached contourpy-1.1.1-cp310-cp310-macosx_10_9_x86_64.whl (247 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Using cached fonttools-4.43.1-cp310-cp310-macosx_10_9_x86_64.whl (2.2 MB)\n",
      "Using cached joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "Using cached kiwisolver-1.4.5-cp310-cp310-macosx_10_9_x86_64.whl (68 kB)\n",
      "Using cached Pillow-10.0.1-cp310-cp310-macosx_10_10_x86_64.whl (3.7 MB)\n",
      "Using cached pyparsing-3.1.1-py3-none-any.whl (103 kB)\n",
      "Using cached scipy-1.11.3-cp310-cp310-macosx_10_9_x86_64.whl (37.3 MB)\n",
      "Using cached threadpoolctl-3.2.0-py3-none-any.whl (15 kB)\n",
      "Using cached fastjsonschema-2.18.1-py3-none-any.whl (23 kB)\n",
      "Using cached zope.interface-6.1-cp310-cp310-macosx_10_9_x86_64.whl (202 kB)\n",
      "Installing collected packages: fastjsonschema, zope.interface, tzdata, threadpoolctl, pyparsing, pillow, numpy, kiwisolver, joblib, fonttools, cycler, scipy, pandas, datetime, contourpy, scikit-learn, matplotlib, seaborn, nbformat\n",
      "Successfully installed contourpy-1.1.1 cycler-0.12.1 datetime-5.2 fastjsonschema-2.18.1 fonttools-4.43.1 joblib-1.3.2 kiwisolver-1.4.5 matplotlib-3.8.0 nbformat-5.9.2 numpy-1.26.0 pandas-2.1.1 pillow-10.0.1 pyparsing-3.1.1 scikit-learn-1.3.1 scipy-1.11.3 seaborn-0.13.0 threadpoolctl-3.2.0 tzdata-2023.3 zope.interface-6.1\n"
     ]
    }
   ],
   "source": [
    "!pip3 install numpy pandas matplotlib seaborn scikit-learn datetime nbformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">/var/folders/3g/lt92y6v536s5sxynst_qrtsm0000gn/T/ipykernel_38194/2620577813.py:</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">17</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> DeprecationWarning</span><span style=\"color: #808000; text-decoration-color: #808000\">: The `airflow.operators.python_operator.PythonOperator` class is deprecated. Please use `</span><span style=\"color: #808000; text-decoration-color: #808000\">'airflow.operators.python.PythonOperator'</span><span style=\"color: #808000; text-decoration-color: #808000\">`.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;33m/var/folders/3g/lt92y6v536s5sxynst_qrtsm0000gn/T/ipykernel_38194/\u001b[0m\u001b[1;33m2620577813.py\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m17\u001b[0m\u001b[1;33m DeprecationWarning\u001b[0m\u001b[33m: The `airflow.operators.python_operator.PythonOperator` class is deprecated. Please use `\u001b[0m\u001b[33m'airflow.operators.python.PythonOperator'\u001b[0m\u001b[33m`.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import math\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import sqlite3\n",
    "from airflow import DAG\n",
    "from airflow.utils.dates import days_ago\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "\n",
    "import nbformat\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Create a requirements.txt file with the necessary packages\n",
    "!pip freeze > airflow/dags/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Player</th>\n",
       "      <th>Nation</th>\n",
       "      <th>Pos</th>\n",
       "      <th>Squad</th>\n",
       "      <th>Age</th>\n",
       "      <th>Born</th>\n",
       "      <th>Starts</th>\n",
       "      <th>Min</th>\n",
       "      <th>Gls</th>\n",
       "      <th>Total_Att</th>\n",
       "      <th>...</th>\n",
       "      <th>Blocks_Pass</th>\n",
       "      <th>Clr</th>\n",
       "      <th>Err</th>\n",
       "      <th>Touches_Touches</th>\n",
       "      <th>Touches_DefPen</th>\n",
       "      <th>Dribbles_Succ</th>\n",
       "      <th>Dribbles_Att</th>\n",
       "      <th>Dribbles_Mis</th>\n",
       "      <th>AerialDuels_Won</th>\n",
       "      <th>AerialDuels_Lost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>Zaira Flores</td>\n",
       "      <td>es ESP</td>\n",
       "      <td>DF,MF</td>\n",
       "      <td>Alhama</td>\n",
       "      <td>30</td>\n",
       "      <td>1993</td>\n",
       "      <td>4</td>\n",
       "      <td>299</td>\n",
       "      <td>0</td>\n",
       "      <td>91</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>129</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>Garazi Fácila</td>\n",
       "      <td>es ESP</td>\n",
       "      <td>DF,MF</td>\n",
       "      <td>Alavés</td>\n",
       "      <td>24</td>\n",
       "      <td>1999</td>\n",
       "      <td>8</td>\n",
       "      <td>720</td>\n",
       "      <td>0</td>\n",
       "      <td>319</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>422</td>\n",
       "      <td>31</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>Geyse</td>\n",
       "      <td>br BRA</td>\n",
       "      <td>FW,MF</td>\n",
       "      <td>Barcelona</td>\n",
       "      <td>25</td>\n",
       "      <td>1998</td>\n",
       "      <td>7</td>\n",
       "      <td>606</td>\n",
       "      <td>5</td>\n",
       "      <td>136</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>228</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>Irina Uribe</td>\n",
       "      <td>es ESP</td>\n",
       "      <td>FW,MF</td>\n",
       "      <td>Levante Planas</td>\n",
       "      <td>25</td>\n",
       "      <td>1998</td>\n",
       "      <td>8</td>\n",
       "      <td>793</td>\n",
       "      <td>4</td>\n",
       "      <td>170</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>268</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>34</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>Yulema Corres</td>\n",
       "      <td>es ESP</td>\n",
       "      <td>MF</td>\n",
       "      <td>Athletic Club</td>\n",
       "      <td>31</td>\n",
       "      <td>1992</td>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Player  Nation    Pos           Squad  Age  Born  Starts  Min  \\\n",
       "113   Zaira Flores  es ESP  DF,MF          Alhama   30  1993       4  299   \n",
       "104  Garazi Fácila  es ESP  DF,MF          Alavés   24  1999       8  720   \n",
       "134          Geyse  br BRA  FW,MF       Barcelona   25  1998       7  606   \n",
       "342    Irina Uribe  es ESP  FW,MF  Levante Planas   25  1998       8  793   \n",
       "81   Yulema Corres  es ESP     MF   Athletic Club   31  1992       1   45   \n",
       "\n",
       "     Gls  Total_Att  ...  Blocks_Pass  Clr  Err  Touches_Touches  \\\n",
       "113    0         91  ...            1    9    0              129   \n",
       "104    0        319  ...            5   27    0              422   \n",
       "134    5        136  ...            8    3    0              228   \n",
       "342    4        170  ...            4    0    0              268   \n",
       "81     0         12  ...            0    0    0               15   \n",
       "\n",
       "     Touches_DefPen  Dribbles_Succ  Dribbles_Att  Dribbles_Mis  \\\n",
       "113               7              0             0             2   \n",
       "104              31              4            12            13   \n",
       "134               1              3            21            21   \n",
       "342               1             14            34            34   \n",
       "81                0              0             0             2   \n",
       "\n",
       "     AerialDuels_Won  AerialDuels_Lost  \n",
       "113                2                 6  \n",
       "104                4                 2  \n",
       "134                0                 3  \n",
       "342                0                 3  \n",
       "81                 1                 2  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(358, 22)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =========================== Random Musings =========================== #\n",
    "spanish_squads = ['Sevilla', 'Sporting Huelva', 'Athletic Club', 'Levante Planas',\n",
    "       'UDG Tenerife', 'Villarreal', 'Madrid CFF', 'Barcelona',\n",
    "       'Atlético Madrid', 'Real Madrid', 'Alhama', 'Alavés',\n",
    "       'Real Sociedad', 'Levante', 'Real Betis', 'Valencia']\n",
    "\n",
    "explanable_cols = ['Player','Nation','Pos','Squad','Age','Born','Starts','Min','Gls','Total_Att','Blocks_Blocks','Blocks_Sh','Blocks_Pass','Clr','Err','Touches_Touches','Touches_DefPen','Dribbles_Succ','Dribbles_Att','Dribbles_Mis','AerialDuels_Won','AerialDuels_Lost']\n",
    "\n",
    "spanish_players = pd.read_csv('assets/all_players.csv')\n",
    "spanish_players = spanish_players[spanish_players['Squad'].isin(spanish_squads)]\n",
    "spanish_players = spanish_players[explanable_cols]\n",
    "\n",
    "# Update the Age column by substracting the Born column from the current year if Born is not null\n",
    "spanish_players['Age'] = spanish_players['Born'].apply(lambda x: (2023- x) if x != np.nan else np.nan)\n",
    "\n",
    "# If the Nation is NaN, replace it with Spain\n",
    "spanish_players['Nation'] = spanish_players['Nation'].fillna('es ESP')\n",
    "\n",
    "# In the spanish_players dataframe, if the datatype is float64, change the datatype to int64\n",
    "for col in spanish_players.columns:\n",
    "    if spanish_players[col].dtype == 'float64':\n",
    "        spanish_players[col] = spanish_players[col].astype('int64')\n",
    "\n",
    "cols = spanish_players.columns.to_list()\n",
    "display(spanish_players.sample(5), spanish_players.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DAG arguments\n",
    "default_args = {\n",
    "    'owner': 'Lagom-QB',\n",
    "    'depends_on_past': False,\n",
    "    'retries': 2,\n",
    "    'retry_delay': timedelta(minutes=5)\n",
    "}\n",
    "\n",
    "dag = DAG(\n",
    "    'ETL_DAG',\n",
    "    description='Load data into SQLite database using Airflow',\n",
    "    default_args=default_args,\n",
    "    schedule_interval=None,\n",
    "    start_date=days_ago(1),\n",
    "    catchup=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract  data\n",
    "file_loc       = 'assets/matches-checkpoint.csv'\n",
    "useless_ids    = ['Away_id','Home_id','Match_id','League_id']\n",
    "spanish_squads = ['Sevilla', 'Sporting Huelva', 'Athletic Club', 'Levante Planas',\n",
    "                  'UDG Tenerife', 'Villarreal', 'Madrid CFF', 'Barcelona',\n",
    "                  'Atlético Madrid', 'Real Madrid', 'Alhama', 'Alavés',\n",
    "                  'Real Sociedad', 'Levante', 'Real Betis', 'Valencia']\n",
    "def extract_data(file_location = file_loc, spanish_squads = spanish_squads, useless_ids = useless_ids):\n",
    "    matches = pd.read_csv(file_location)\n",
    "    # Make sure 'Home' or 'Away' is in the spanish_squads\n",
    "    matches = matches[(matches['Home'].isin(spanish_squads)) | (matches['Away'].isin(spanish_squads))]\n",
    "    matches = matches.drop(useless_ids, axis=1).reset_index(drop=True)\n",
    "    matches['Date'] = pd.to_datetime(matches['Date'])\n",
    "    \n",
    "    return matches\n",
    "\n",
    "# matches = extract_data()\n",
    "# display(matches.sample(5), matches.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Transformation\n",
    "\n",
    "**Feature Engineering** : Calculate derived metrics\n",
    "\n",
    "**Data Aggregation** : Aggregate the data to a higher level of granularity.\n",
    "\n",
    "**Data Filtering** : Filter the dataframe based on specific conditions or criteria. \n",
    "\n",
    "**Data Transformation** : Apply mathematical or statistical transformations to the data. \n",
    "\n",
    "**Feature Scaling** : Scale the numeric features to a common range to avoid bias in the analysis.\n",
    "\n",
    "**Data Encoding** : Encode categorical variables into numerical representations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform task\n",
    "def transform_data(matches):\n",
    "    # Clean data\n",
    "    matches = matches.dropna()\n",
    "\n",
    "    # Convert data types\n",
    "    matches['Date'] = pd.to_datetime(matches['Date'])\n",
    "    matches['Time'] = pd.to_datetime(matches['Time'], format='%H:%M').dt.time\n",
    "\n",
    "    # Goal Difference: You can calculate the goal difference by subtracting the \"ScoreAway\" from the \"ScoreHome\" column. This metric gives you the difference in goals scored between the home and away teams in each match.\n",
    "    matches['GoalDifference'] = matches['ScoreHome'] - matches['ScoreAway']\n",
    "    \n",
    "    # Result: You can calculate the result of each match by comparing the \"ScoreHome\" and \"ScoreAway\" columns. If the home team scored more goals than the away team, then the home team won the match. If the home team scored fewer goals than the away team, then the home team lost the match. If both teams scored the same number of goals, then the match was a draw.\n",
    "    matches['Result'] = matches['Score'].apply(lambda x: 'Win' if x[0] > x[2] else 'Draw' if x[0] == x[2] else 'Loss')\n",
    "    \n",
    "    # Expected Goals Difference: Similar to the goal difference, you can calculate the expected goals difference by subtracting the \"xGAway\" from the \"xGHome\" column. This metric represents the difference in expected goals between the home and away teams in each match.\n",
    "    matches['ExpectedGoalDifference'] = matches['xGHome'] - matches['xGAway']\n",
    "    \n",
    "    # Points: You can calculate the points earned by each team using a scoring system (e.g., 3 points for a win, 1 point for a draw, and 0 points for a loss). You can create a new column called \"Points\" and assign the corresponding points based on the match result in the \"Score\" column.\n",
    "    matches['Points'] = matches['Score'].apply(lambda x: 3 if x[0] > x[2] else 1 if x[0] == x[2] else 0)\n",
    "    \n",
    "    # Expected Points: Similar to the points metric, you can calculate the expected points earned by each team using a similar scoring system but based on the expected goals (e.g., 3 points for xGHome > xGAway, 1 point for xGHome = xGAway, and 0 points for xGHome < xGAway). You can create a new column called \"ExpectedPoints\" and assign the corresponding expected points based on the expected goals in the \"xGHome\" and \"xGAway\" columns.\n",
    "    matches['ExpectedPoints'] = matches['Score'].apply(lambda x: 3 if x[0] > x[2] else 1 if x[0] == x[2] else 0)\n",
    "    \n",
    "    # Win Percentage: You can calculate the win percentage for each team by dividing the number of wins (based on the \"Score\" column) by the total number of matches played.\n",
    "    wins = matches[matches['Result'] == 'Win'].groupby('Home').size()\n",
    "    total_matches = matches.groupby('Home').size()\n",
    "    win_percentage = (wins / total_matches) * 100\n",
    "    # Add win percentage to matches dataframe\n",
    "    matches['WinPercentage'] = matches['Home'].map(win_percentage)\n",
    "\n",
    "    matches['TotalGoals'] = matches['ScoreHome'] + matches['ScoreAway']\n",
    "\n",
    "    matches['xGRatio'] = matches['xGHome'] / (matches['xGHome'] + matches['xGAway'])\n",
    "\n",
    "    def get_points(row):\n",
    "        if row['Result'] == 'Win':\n",
    "            return 3\n",
    "        elif row['Result'] == 'Draw':\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    matches['Points'] = matches.apply(get_points, axis=1)\n",
    "\n",
    "    return matches\n",
    "\n",
    "# matches = transform_data(matches)\n",
    "# display(matches.sample(5), matches.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ETL Process and Data Integration\n",
    "_Loading of the data_ . \n",
    "__Apache Airflow__ supports a few databases: \n",
    "- SQLite _Lightweight filebased database suitable for small-scale deployments and testing_\n",
    "- PostgreSQL _Relational database widely used in production environments_\n",
    "- MySQL _Popular relational database widely used_\n",
    "- Microsoft SQL Server _Commercial relational database widely used in enterprises_\n",
    "- Oracle _Commercial relational database widely used in enterprises_\n",
    "- Amazon RedShift _Cloud-based data warehouse optimized for analytics workloads_\n",
    "- Google BigQuery _Cloud-based data warehouse optimized for analytics workloads_\n",
    "- Apache Casssandra _Distributed No-SQL database optimized for high scalability and availability_\n",
    "- Apache Hive _Data warehouse infrastructure for data summarization, querying and analytics_\n",
    "\n",
    "I'm using SQLite because it's a small scale dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load task\n",
    "def load_data(matches):\n",
    "    # Connect to database\n",
    "    conn = sqlite3.connect('assets/spanish_matches.db')\n",
    "\n",
    "    # Create cursor\n",
    "    c = conn.cursor()\n",
    "\n",
    "    # Create table\n",
    "    c.execute(\"\"\"CREATE TABLE IF NOT EXISTS matches (\n",
    "        Wk INTERGER,\n",
    "        Day TEXT,\n",
    "        Date DATE,\n",
    "        Time TIME,\n",
    "        Home TEXT,\n",
    "        xGHome FLOAT,\n",
    "        Score TEXT,\n",
    "        xGAway FLOAT,\n",
    "        Away TEXT,\n",
    "        xPHome FLOAT,\n",
    "        xPAway FLOAT,\n",
    "        ScoreHome INTERGER,\n",
    "        ScoreAway INTERGER,\n",
    "        GoalDifference INTERGER,\n",
    "        Result TEXT,\n",
    "        ExpectedGoalDifference FLOAT,\n",
    "        Points INTERGER,\n",
    "        ExpectedPoints INTERGER,\n",
    "        WinPercentage FLOAT,\n",
    "        TotalGoals INTERGER,\n",
    "        xGRatio FLOAT\n",
    "    )\"\"\")\n",
    "\n",
    "    # Insert DataFrame records one by one.\n",
    "    for i, row in matches.iterrows():\n",
    "        c.execute(\"\"\"INSERT INTO matches VALUES (?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?)\"\"\", (\n",
    "            row['Wk'],\n",
    "            row['Day'],\n",
    "            row['Date'],\n",
    "            row['Time'],\n",
    "            row['Home'],\n",
    "            row['xGHome'],\n",
    "            row['Score'],\n",
    "            row['xGAway'],\n",
    "            row['Away'],\n",
    "            row['xPHome'],\n",
    "            row['xPAway'],\n",
    "            row['ScoreHome'],\n",
    "            row['ScoreAway'],\n",
    "            row['GoalDifference'],\n",
    "            row['Result'],\n",
    "            row['ExpectedGoalDifference'],\n",
    "            row['Points'],\n",
    "            row['ExpectedPoints'],\n",
    "            row['WinPercentage'],\n",
    "            row['TotalGoals'],\n",
    "            row['xGRatio']\n",
    "        ))\n",
    "\n",
    "    # Commit changes\n",
    "    conn.commit()\n",
    "\n",
    "    # Close cursor and connection\n",
    "    c.close()\n",
    "    conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign tasks\n",
    "extract_task = PythonOperator(\n",
    "    task_id='extract_data',\n",
    "    python_callable=extract_data,\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "transform_task = PythonOperator(\n",
    "    task_id='transform_data',\n",
    "    python_callable=transform_data,\n",
    "    op_kwargs={'matches': '{{ ti.xcom_pull(task_ids=\"extract_data\") }}'},\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "load_task = PythonOperator(\n",
    "    task_id='load_data',\n",
    "    python_callable=load_data,\n",
    "    op_kwargs={'matches': '{{ ti.xcom_pull(task_ids=\"transform_data\") }}'},\n",
    "    dag=dag\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no such file or directory: /Users/iffiness/airflow_env/bin/airflow\n",
      "zsh:1: no such file or directory: /Users/iffiness/airflow_env/bin/airflow\n",
      "zsh:1: no such file or directory: /Users/iffiness/airflow_env/bin/airflow\n",
      "zsh:1: no such file or directory: /Users/iffiness/airflow_env/bin/airflow\n"
     ]
    }
   ],
   "source": [
    "!~/airflow_env/bin/airflow scheduler -D\n",
    "!~/airflow_env/bin/airflow webserver -D\n",
    "\n",
    "!~/airflow_env/bin/airflow dags list\n",
    "\n",
    "!~/airflow_env/bin/airflow cheat-sheet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation and Quality Assurance\n",
    "\n",
    "To validate the quality of the data, I'm connecting to the database to check for null values in each column of the matches table.  \n",
    "Specifically, I'll check the data type, the range anf completeness of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Task(PythonOperator): validate_data>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validate data in the database and ensure the proper quality\n",
    "def validate_data():\n",
    "    # Connect to database\n",
    "    conn = sqlite3.connect('assets/spanish_matches.db')\n",
    "\n",
    "    # Create cursor\n",
    "    c = conn.cursor()\n",
    "\n",
    "    # Data type validation\n",
    "    c.execute(\"\"\"SELECT COUNT(*) FROM matches where CAST(Wk AS INTEGER) IS NULL\"\"\")\n",
    "    null_count = c.fetchone()[0]\n",
    "    if null_count == 0:\n",
    "        print('Data type validation passed.')\n",
    "    else:\n",
    "        print(f'Data type validation failed with {null_count} null values.')\n",
    "\n",
    "    # Data range validation\n",
    "    c.execute(\"\"\"SELECT COUNT(*) FROM matches where Wk < 1 OR Wk > 10\"\"\")\n",
    "    range_count = c.fetchone()[0]\n",
    "    if range_count == 0:\n",
    "        print('Data range validation passed.')\n",
    "    else:\n",
    "        print(f'Data range validation failed with {range_count} values out of range.')\n",
    "\n",
    "    # Data completeness validation\n",
    "    c.execute(\"\"\"SELECT COUNT(*) FROM matches where Wk IS NULL\"\"\")\n",
    "    completeness_count = c.fetchone()[0]\n",
    "    if completeness_count == 0:\n",
    "        print('Data completeness validation passed.')\n",
    "    else:\n",
    "        print(f'Data completeness validation failed with {completeness_count} null values.')\n",
    "\n",
    "    c.close()\n",
    "    conn.close()\n",
    "\n",
    "validate_task = PythonOperator(\n",
    "    task_id='validate_data',\n",
    "    python_callable=validate_data,\n",
    "    op_kwargs={'matches': '{{ ti.xcom_pull(task_ids=\"load_data\") }}'},\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "# Define task dependencies\n",
    "extract_task >> transform_task >> load_task >> validate_task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reporting and Analysis\n",
    "\n",
    "Generate meaningful insights and reports.\n",
    "- Trend analysis\n",
    "- Team Performance analysis\n",
    "- Team comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Task(PythonOperator): trend_analysis>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trend analysis\n",
    "def trend_analysis():\n",
    "    # Connect to database\n",
    "    conn = sqlite3.connect('assets/spanish_matches.db')\n",
    "\n",
    "    # Create cursor\n",
    "    c = conn.cursor()\n",
    "\n",
    "    # Data type validation\n",
    "    c.execute(\"\"\"SELECT * FROM matches\"\"\")\n",
    "    matches = pd.DataFrame(c.fetchall())\n",
    "    \n",
    "    # Define plot function\n",
    "    def plot_data():\n",
    "        sns.lineplot(x='Date', y='TotlaGoals', data=matches)\n",
    "        plt.title('Total Goals Scored')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Total Goals')\n",
    "        plt.show()\n",
    "    # Look at the correlation between the expected goals and the actual goals\n",
    "    def calculate_correlation():\n",
    "        corr_home = matches['xGHome'].corr(matches['ScoreHome'])\n",
    "        print(f'Correlation between expected Goals for the Home and actual goals Home: {corr_home}')\n",
    "        corr_away = matches['xGAway'].corr(matches['ScoreAway'])\n",
    "        print(f'Correlation between expected Goals for the Away and actual goals Away: {corr_away}')\n",
    "    \n",
    "    plot_data()\n",
    "    calculate_correlation()\n",
    "\n",
    "trend_analysis_task = PythonOperator(\n",
    "    task_id='trend_analysis',\n",
    "    python_callable=trend_analysis,\n",
    "    op_kwargs={'matches': '{{ ti.xcom_pull(task_ids=\"validate_data\") }}'},\n",
    "    dag=dag\n",
    ")   \n",
    "\n",
    "# Define task dependencies\n",
    "extract_task >> transform_task >> load_task >> validate_task >> trend_analysis_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no such file or directory: /Users/iffiness/airflow_env/bin/airflow\n"
     ]
    }
   ],
   "source": [
    "# Exectute the DAG workflow and view the results in the Airflow UI from scripts/DataPipelining.py\n",
    "!~/airflow_env/bin/airflow trigger_dag "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook DataPipelining.ipynb to script\n",
      "[NbConvertApp] Writing 15499 bytes to airflow/dags/DataPipelining.py\n"
     ]
    }
   ],
   "source": [
    "# Convert notebook to python script\n",
    "!jupyter nbconvert --to script DataPipelining.ipynb --output-dir='airflow/dags/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
